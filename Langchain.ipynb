{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXiEiJ+4ljdsOzMro0m3ZB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Karan-Baid/rag_qa_pdf/blob/main/Langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pymupdf\n",
        "import json\n",
        "import os\n",
        "from math import ceil\n",
        "\n",
        "def clean_bytes(data_obj):\n",
        "    \"\"\"\n",
        "    Recursively traverses a dictionary or list to convert bytes to strings.\n",
        "    This prevents JSON serialization errors with raw binary data from PyMuPDF.\n",
        "    \"\"\"\n",
        "    if isinstance(data_obj, dict):\n",
        "        # If it's a dictionary, recursively clean each value\n",
        "        return {key: clean_bytes(value) for key, value in data_obj.items()}\n",
        "    elif isinstance(data_obj, list):\n",
        "        # If it's a list, recursively clean each item\n",
        "        return [clean_bytes(item) for item in data_obj]\n",
        "    elif isinstance(data_obj, bytes):\n",
        "        # If it's bytes, represent it as a placeholder string\n",
        "        # This is safer than attempting to decode potentially non-textual data\n",
        "        return \"<binary data>\"\n",
        "    else:\n",
        "        # If it's any other type (int, str, float), return it as is\n",
        "        return data_obj\n",
        "\n",
        "# --- Configuration ---\n",
        "# Replace with your PDF's file path\n",
        "pdf_path = \"E0H1CM114.pdf\"\n",
        "# Name of the single output JSON file that will contain all page data\n",
        "output_path = \"metadata.json\"\n",
        "# --- End of Configuration ---\n",
        "\n",
        "# Open the PDF document\n",
        "try:\n",
        "    doc = pymupdf.open(pdf_path)\n",
        "except Exception as e:\n",
        "    print(f\"Error opening PDF {pdf_path}: {e}\")\n",
        "    exit()\n",
        "\n",
        "# This list will hold the metadata for every page\n",
        "all_pages_data = []\n",
        "\n",
        "# --- Iterate through every page in the document ---\n",
        "for page_num in range(len(doc)):\n",
        "    page = doc[page_num]\n",
        "    print(f\"Processing page {page_num + 1}/{len(doc)}...\")\n",
        "\n",
        "    # 1. Extract the raw page data as a dictionary\n",
        "    page_data = page.get_text(\"dict\")\n",
        "\n",
        "    # 2. Perform a deep clean to handle non-serializable bytes\n",
        "    cleaned_page_data = clean_bytes(page_data)\n",
        "\n",
        "    # 3. Add the cleaned page data to our master list\n",
        "    all_pages_data.append({\n",
        "        \"page_number\": page_num + 1,\n",
        "        \"content\": cleaned_page_data\n",
        "    })\n",
        "\n",
        "\n",
        "# 4. Create the final output object\n",
        "final_output = {\n",
        "    \"document_metadata\": {\n",
        "        \"source_pdf\": os.path.basename(pdf_path),\n",
        "        \"total_pages\": len(doc)\n",
        "    },\n",
        "    \"pages\": all_pages_data\n",
        "}\n",
        "\n",
        "# 5. Save the consolidated data to a single JSON file\n",
        "with open(output_path, \"w\", encoding='utf-8') as f:\n",
        "    json.dump(final_output, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "with open(output_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Dictionary to hold best merged text for each key\n",
        "text_dict = {}\n",
        "\n",
        "# Smart merge function to handle partial overlaps\n",
        "def smart_merge(existing_text, new_text):\n",
        "    existing_text = existing_text.strip()\n",
        "    new_text = new_text.strip()\n",
        "\n",
        "    # Case 1: new is already inside existing\n",
        "    if new_text in existing_text:\n",
        "        return existing_text\n",
        "\n",
        "    # Case 2: existing is inside new → replace\n",
        "    if existing_text in new_text:\n",
        "        return new_text\n",
        "\n",
        "    # Case 3: partial overlap → try to merge\n",
        "    # We'll try to find the longest suffix of existing that matches prefix of new\n",
        "    max_overlap = 0\n",
        "    min_len = min(len(existing_text), len(new_text))\n",
        "    for i in range(1, min_len):\n",
        "        if existing_text[-i:] == new_text[:i]:\n",
        "            max_overlap = i\n",
        "\n",
        "    # Merge using that overlap\n",
        "    merged = existing_text + new_text[max_overlap:]\n",
        "    return merged\n",
        "\n",
        "# Iterate through the JSON\n",
        "for page in data[\"pages\"]:\n",
        "    page_num = page[\"page_number\"]\n",
        "\n",
        "    for block in page[\"content\"][\"blocks\"]:\n",
        "        if \"lines\" not in block:\n",
        "            continue\n",
        "\n",
        "        for line in block[\"lines\"]:\n",
        "            for span in line[\"spans\"]:\n",
        "                key = (\n",
        "                    page_num,\n",
        "                    span[\"size\"],\n",
        "                    span[\"ascender\"],\n",
        "                    span[\"descender\"],\n",
        "                    span.get(\"font\", \"\")\n",
        "                )\n",
        "                text = span[\"text\"].strip()\n",
        "\n",
        "                if key in text_dict:\n",
        "                    text_dict[key] = smart_merge(text_dict[key], text)\n",
        "                else:\n",
        "                    text_dict[key] = text\n",
        "\n",
        "\n",
        "sorted_items = sorted(text_dict.items(), key=lambda item: item[0][1], reverse=True)\n",
        "unique_sizes = []\n",
        "seen = set()\n",
        "for (page, size, asc, desc, font), text in sorted_items:\n",
        "    if size not in seen:\n",
        "        unique_sizes.append(size)\n",
        "        seen.add(size)\n",
        "    if len(unique_sizes) == 4:  # title + H1 + H2 + H3\n",
        "        break\n",
        "\n",
        "# Step 4: Compute ranges\n",
        "title_size = unique_sizes[0]\n",
        "h1_base = ceil(unique_sizes[1] / 5.0) * 5\n",
        "\n",
        "heading_ranges = {\n",
        "    \"H1\": (h1_base - 4, h1_base),\n",
        "    \"H2\": (h1_base - 9, h1_base - 5),\n",
        "    \"H3\": (h1_base - 14, h1_base - 10)\n",
        "}\n",
        "\n",
        "# Step 5: Generate output\n",
        "output = {\n",
        "    \"title\": \"\",\n",
        "    \"outline\": []\n",
        "}\n",
        "\n",
        "for (page, size, asc, desc, font), text in sorted_items:\n",
        "    if size == title_size and output[\"title\"] == \"\":\n",
        "        output[\"title\"] = text\n",
        "        continue\n",
        "\n",
        "    for level, (low, high) in heading_ranges.items():\n",
        "        if low <= size <= high:\n",
        "            output[\"outline\"].append({\n",
        "                \"level\": level,\n",
        "                \"text\": text,\n",
        "                \"page\": page\n",
        "            })\n",
        "            break\n",
        "final_op_path=\"output.json\"\n",
        "# print(json.dumps(output, indent=2))\n",
        "with open(final_op_path, \"w\", encoding='utf-8') as f:\n",
        "    json.dump(output, f, indent=4, ensure_ascii=False)\n"
      ],
      "metadata": {
        "id": "nexdx7NHqAek",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c1c9817-e14c-4198-f1e2-5d8d440e4b57"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing page 1/14...\n",
            "Processing page 2/14...\n",
            "Processing page 3/14...\n",
            "Processing page 4/14...\n",
            "Processing page 5/14...\n",
            "Processing page 6/14...\n",
            "Processing page 7/14...\n",
            "Processing page 8/14...\n",
            "Processing page 9/14...\n",
            "Processing page 10/14...\n",
            "Processing page 11/14...\n",
            "Processing page 12/14...\n",
            "Processing page 13/14...\n",
            "Processing page 14/14...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADCWHSpLalZo",
        "outputId": "c1c63b37-471e-43ca-da2a-ebfecba20f5a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.26.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "opHXV30FapG6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}